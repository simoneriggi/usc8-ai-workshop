{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1806e013",
   "metadata": {},
   "source": [
    "# Outline\n",
    "This is a tutorial showing how to create, train and test a CNN classifier for astronomical image data using PyTorch:\n",
    "\n",
    "- `Dataset`: Galaxy MNIST, galaxies of different morphologies observed in 3 optical bands\n",
    "- `CNN models`: custom & ResNet\n",
    "- `Task`: classify input images into 4 possible classes\n",
    "\n",
    "The tutorial will cover these steps:\n",
    "\n",
    "1) Set up the environment\n",
    "2) Download the dataset and create data loaders and transformers/augmenters\n",
    "3) Create a configurable CNN classifier\n",
    "4) Train the classifier\n",
    "5) Evaluate the classifier on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df2d01",
   "metadata": {},
   "source": [
    "# Configuring the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda7e73",
   "metadata": {},
   "source": [
    "## Module installation\n",
    "Let's first install the python modules required for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f608540",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##   IMGPROC MODULES\n",
    "###########################\n",
    "%pip install -q pillow opencv-python\n",
    "\n",
    "###########################\n",
    "##   ML MODULES\n",
    "###########################\n",
    "%pip install -q torch torchvision torchmetrics torchsummary scikit-learn tqdm\n",
    "%pip install -q wandb -qqq\n",
    "%pip install -q grad-cam\n",
    "\n",
    "###########################\n",
    "##   OTHER MODULES\n",
    "###########################\n",
    "%pip install -q shortuuid\n",
    "%pip install -q gdown # gDrive\n",
    "%pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91353df9",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ac7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##   STANDARD MODULES\n",
    "###########################\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gdown\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from itertools import islice\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "\n",
    "###########################\n",
    "##   IMGPROC/TORCH MODULES\n",
    "###########################\n",
    "# - Image proc\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# - Torch modules\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Subset, random_split\n",
    "import torchvision\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchmetrics\n",
    "from torchsummary import summary\n",
    "\n",
    "# - GradCAM\n",
    "from pytorch_grad_cam import (\n",
    "  GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus,\n",
    "  AblationCAM, XGradCAM, EigenCAM, EigenGradCAM,\n",
    "  LayerCAM, FullGrad, GradCAMElementWise, KPCA_CAM\n",
    ")\n",
    "from pytorch_grad_cam.utils.find_layers import find_layer_types_recursive\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eab38b",
   "metadata": {},
   "source": [
    "## Project folders\n",
    "We create a working directory where to run the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc372736",
   "metadata": {},
   "outputs": [],
   "source": [
    "topdir= os.getcwd()\n",
    "rundir= os.path.join(topdir, \"run-gmnist_classifier\")\n",
    "path = Path(rundir)\n",
    "path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5186fc69",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For this tutorial, we are going to use the [Galaxy MNIST dataset](https://github.com/mwalmsley/galaxy_mnist). \n",
    "\n",
    "The dataset currently contains 10,000 images of galaxies in three optical bands (grz), either 64x64x3 (low reso) or 224x224x3 (high reso), taken from the Dark Energy Camera Legacy Survey (DECaLS) Galaxy Zoo project.\n",
    "DECaLS uses the Dark Energy Camera (DECam) at the 4m Blanco telescope in Chile. Fluxes in the grz bands were converted to RGB colours (see Section 2.3 of reference paper) and PNG images were created for each sample galaxy.   \n",
    "\n",
    "The dataset is split into two subsets: \n",
    "\n",
    "- train: 8000 images\n",
    "- test: 2000 images\n",
    "\n",
    "The dataset contains 4 possible classes of galaxy morphologies:\n",
    "\n",
    "- SMOOTH_ROUND: smooth and round galaxy. Should not have signs of spires.   \n",
    "- SMOOTH_CIGAR: smooth and cigar-shaped galaxy, looks like being seen edge on. This should not have signs of spires of a spiral galaxy.\n",
    "- EDGE_ON_DISK: edge-on-disk/spiral galaxy. This disk galaxy should have signs of spires, as seen from an edge-on perspective.\n",
    "- UNBARRED_SPIRAL: unbarred spiral galaxy. Has signs of a disk and/or spires\n",
    "\n",
    "Note that categories SMOOTH_CIGAR and EDGE_ON_DISK classes tend to be very similar to each other. To categorize them, ask yourself the following question: Is this galaxy very smooth, maybe with a small bulge? Then it belongs to class SMOOTH_CIGAR. Does it have irregularities/signs of structure? Then it belongs to class EDGE_ON_DISK.\n",
    "\n",
    "In this tutorial we are going to use the high-reso images (224x224). The original dataset format was slighly modified. The modified dataset is available for download in Google Drive.\n",
    "\n",
    "More details on the observational data and labelling are available in these references:\n",
    "\n",
    "- [Galaxy Zoo DECaLS paper](https://ui.adsabs.harvard.edu/abs/2022MNRAS.509.3966W/abstract)    \n",
    "- [Galaxy Zoo DECaLS data](https://zenodo.org/records/4573248)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa8bb8",
   "metadata": {},
   "source": [
    "## Dataset Download\n",
    "We download the dataset from GoogleDrive URL and unzip it in the main folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Set dataset URL & paths\n",
    "dataset_name= \"galaxy_mnist-dataset\"\n",
    "dataset_dir= os.path.join(rundir, dataset_name)\n",
    "dataset_tar= 'galaxy_mnist-dataset.tar.gz'\n",
    "dataset_tar_fullpath= os.path.join(rundir, dataset_tar)\n",
    "dataset_url= 'https://drive.google.com/uc?export=download&id=1OprJ_NQIFyQSRWqjGLFQsAMumHvJ-tMB'\n",
    "\n",
    "# - Download dataset (if not previously downloaded)\n",
    "if not os.path.isfile(dataset_tar_fullpath):\n",
    "  print(\"Downloading file from url %s ...\" % (dataset_url))\n",
    "  gdown.download(dataset_url, dataset_tar, quiet=False)  \n",
    "  print(\"DONE!\")\n",
    "\n",
    "# - Untar dataset\n",
    "if not os.path.isdir(dataset_dir):\n",
    "  print(\"Unzipping dataset file %s ...\" % (dataset_tar))\n",
    "  fp= tarfile.open(dataset_tar)\n",
    "  fp.extractall('.')\n",
    "  fp.close()   \n",
    "  print(\"DONE!\")\n",
    "\n",
    "# - Moving data to rundir\n",
    "if not os.path.isfile(dataset_tar_fullpath):\n",
    "  print(\"Moving tar file to rundir %s ...\" % (rundir))\n",
    "  shutil.move(dataset_tar, rundir)\n",
    "\n",
    "if not os.path.isdir(dataset_dir):\n",
    "  print(\"Moving datadir to rundir %s ...\" % (rundir))    \n",
    "  shutil.move(dataset_name, rundir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b9117",
   "metadata": {},
   "source": [
    "The dataset provides datalists for the train and test samples in a json format, for 1-channel (channel-averaged data) or 3-channel images. We are going to use the 3-channel data samples:\n",
    "\n",
    "- `train/3chan/datalist_train.json`\n",
    "- `test/3chan/datalist_test.json`\n",
    "\n",
    "Datalists have these format:   \n",
    "    \n",
    "\n",
    "```json\n",
    "{    \n",
    "  \"data\": [    \n",
    "    {    \n",
    "      \"filepaths\": [\n",
    "        \"galaxy_mnist-dataset/train/3chan/train_1.png\"\n",
    "      ],\n",
    "      \"sname\": \"S1\",\n",
    "      \"id\": 1,\n",
    "      \"label\": \"smooth_cigar\"\n",
    "    },    \n",
    "    ...\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e82cb76",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset\n",
    "We create a custom pytorch dataset for the Galaxy MNIST data using pytorch `Dataset` base class. For this we need to override these base methods:    \n",
    "\n",
    "\n",
    "```__len__```: returning the size of the dataset.    \n",
    "```__getitem__```: returning the i-th dataset sample (image and target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151932d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMNISTDataset(Dataset):\n",
    "  \"\"\" Galaxy MNIST dataset \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self, \n",
    "      metadata_file: Optional[Union[str, Path]] = \"\",\n",
    "      subset: Optional[Subset] = None,\n",
    "      transform: Optional[Callable] = None,\n",
    "      target_transform: Optional[Callable] = None,\n",
    "      data_path: Optional[Union[str, Path]] = \"\",\n",
    "  ):\n",
    "    # - Read metadata\n",
    "    self.data_path= data_path\n",
    "    self.subset= subset\n",
    "    if self.subset is None:\n",
    "        print(\"Reading dataset metadata from file %s ...\" % (metadata_file))\n",
    "        self.__read_metadata(metadata_file)\n",
    "        \n",
    "    # - Set pars\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform\n",
    "    self.pil2tensor = T.Compose([T.PILToTensor()]) # no normalization\n",
    "    \n",
    "    self.target2label= {\n",
    "      0: \"smooth_round\",  \n",
    "      1: \"smooth_cigar\",\n",
    "      2: \"edge_on_disk\",\n",
    "      3: \"unbarred_spiral\"\n",
    "    }\n",
    "\n",
    "  def __read_metadata(self, filename):\n",
    "    \"\"\" Read json metadata \"\"\"\n",
    "    \n",
    "    f= open(filename, \"r\")\n",
    "    self.datalist= json.load(f)[\"data\"]\n",
    "  \n",
    "  def __len__(self):\n",
    "    \"\"\" Return size of dataset \"\"\" \n",
    "    if self.subset:\n",
    "      return len(self.subset) \n",
    "    else:\n",
    "      return len(self.datalist)\n",
    "   \n",
    "  def __load_item(self, idx):\n",
    "    \"\"\" Load dataset item \"\"\"\n",
    "    \n",
    "    # - Read image path & class id\n",
    "    img_path= self.datalist[idx]['filepaths'][0]\n",
    "    if self.data_path!=\"\" and os.path.isdir(self.data_path):\n",
    "      img_path= os.path.join(self.data_path, img_path)\n",
    "    \n",
    "    target= self.datalist[idx]['id'] # class id\n",
    "    \n",
    "    # - Read PIL image as RGB\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    return img, target\n",
    "    \n",
    "  def __load_subset_item(self, idx):\n",
    "    \"\"\" Load dataset subset item \"\"\"\n",
    "    \n",
    "    # - Get item from subset\n",
    "    #   NB: img is a tensor\n",
    "    return self.subset[idx]\n",
    "\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\" Return dataset item \"\"\"\n",
    "    \n",
    "    # - Load image/label\n",
    "    if self.subset is None:\n",
    "      img, target= self.__load_item(idx)\n",
    "    else:\n",
    "      img, target= self.subset[idx]\n",
    "\n",
    "    # - Convert PIL to tensor?\n",
    "    if isinstance(img, PIL.Image.Image):\n",
    "      img= self.pil2tensor(img)\n",
    "    \n",
    "    # - Transform img/tensor?\n",
    "    if self.transform is not None:\n",
    "      img = self.transform(img)\n",
    "\n",
    "    # - Transform target?\n",
    "    if self.target_transform is not None:\n",
    "      target = self.target_transform(target)\n",
    "       \n",
    "    return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce61db",
   "metadata": {},
   "source": [
    "## Create data custom transforms\n",
    "We define here a series of custom image transformations that we will apply to the data as augmentations. To do that just create a `nn.Module` and override the `forward` method, like the examples below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef900720",
   "metadata": {},
   "source": [
    "### Random flip\n",
    "A transform that flip either image horizontally/vertically or leave image unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFlip(torch.nn.Module):\n",
    "  \"\"\" Flip image \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, img):\n",
    "    op= random.choice([1,2,3])\n",
    "    if op==1:\n",
    "      return TF.hflip(img)\n",
    "    elif op==2:\n",
    "      return TF.vflip(img)\n",
    "    else:\n",
    "      return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e95f6",
   "metadata": {},
   "source": [
    "### Random rotate\n",
    "A transform that randomly rotate image by 90 degrees step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotate90(torch.nn.Module):\n",
    "  \"\"\"Rotate by one of the given angles: 90, 270, \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, img):\n",
    "    op= random.choice([1,2,3,4])\n",
    "    if op==1:\n",
    "      return TF.rotate(img, 90)\n",
    "    elif op==2:\n",
    "      return TF.rotate(img, 180)\n",
    "    elif op==3:\n",
    "      return TF.rotate(img, 270)\n",
    "    elif op==4:\n",
    "      return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500014a",
   "metadata": {},
   "source": [
    "### Sanitization\n",
    "A transform that set NaNs/inf pixels to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c77c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sanitization(torch.nn.Module):\n",
    "  \"\"\" Set NaN/inf pixels to 0 \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "   \n",
    "  def forward(self, img):\n",
    "    # - Create mask of non-nans pixels    \n",
    "    cond= torch.isfinite(img)\n",
    "    \n",
    "    # - Set nans to 0\n",
    "    img[~cond]= 0\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63266e3d",
   "metadata": {},
   "source": [
    "### Absolute Channel Maximum Scaling\n",
    "This transform finds, for each image, the absolute maximum, and then it scales all channels by this value, taking into account any possible band flux ratio information as sensitive classification variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsChanMaxScaling(torch.nn.Module):\n",
    "  \"\"\" Scale tensor by absolute channel maximum \"\"\"\n",
    "\n",
    "  def __init__(self):    \n",
    "    super().__init__()\n",
    "   \n",
    "  def forward(self, img):\n",
    "    \n",
    "    # - Compute absolute image max across channels\n",
    "    ndim= img.ndim\n",
    "    if ndim==4: # [BATCH,CHAN,Ny,Nx]\n",
    "      img_absmax= torch.amax(img, dim=(1,2,3), keepdim=True)\n",
    "    elif ndim==3: # [CHAN,Ny,Nx]\n",
    "      img_absmax= torch.amax(img, dim=(0,1,2), keepdim=True)\n",
    "    else:\n",
    "      logger.warn(\"Unexpected ndim (%d), returning same image ...\" % (ndim))\n",
    "      return img\n",
    "    \n",
    "    # - Scale image by absmax\n",
    "    img_scaled= img/img_absmax\n",
    "    \n",
    "    return img_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545dcd4",
   "metadata": {},
   "source": [
    "### My custom transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE YOUR OWN TRANSFORM HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40348d7",
   "metadata": {},
   "source": [
    "### Define composite transforms\n",
    "Let's define two composite transforms: one for training data, having standard plus additional augmenter transforms, and the other for validation/test data, having standard transforms.\n",
    "\n",
    "Standard transforms are:\n",
    "\n",
    "- Sanitization\n",
    "- Image resize\n",
    "- Intra-channel normalization\n",
    "- Image sample normalization (optional)\n",
    "\n",
    "Augmenter transforms are:\n",
    "\n",
    "- Random flipping\n",
    "- Random rotation 90 deg\n",
    "- Random crop and resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Define dataset transforms\n",
    "img_resize= 224\n",
    "\n",
    "transform_train= T.Compose(\n",
    "  [  \n",
    "    Sanitization(),  \n",
    "    T.Resize(img_resize, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    RandomFlip(),\n",
    "    RandomRotate90(),  \n",
    "    T.RandomResizedCrop(img_resize, scale=(0.5, 1.0), ratio=(1., 1.), interpolation=T.InterpolationMode.BICUBIC),\n",
    "    AbsChanMaxScaling(), \n",
    "    #T.ToTensor(),# convert to tensor in range [0,1]\n",
    "  ]\n",
    ")\n",
    "\n",
    "transform_imagenet_train= T.Compose(\n",
    "  [  \n",
    "    Sanitization(),  \n",
    "    T.Resize(img_resize, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    RandomFlip(),\n",
    "    RandomRotate90(), \n",
    "    T.RandomResizedCrop(img_resize, scale=(0.5, 1.0), ratio=(1., 1.), interpolation=T.InterpolationMode.BICUBIC),\n",
    "    AbsChanMaxScaling(),  \n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))  \n",
    "  ]\n",
    ")\n",
    "\n",
    "transform= T.Compose(\n",
    "  [\n",
    "    Sanitization(),  \n",
    "    T.Resize(img_resize, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    AbsChanMaxScaling()\n",
    "  ]\n",
    ")\n",
    "\n",
    "transform_imagenet= T.Compose(\n",
    "  [\n",
    "    Sanitization(),  \n",
    "    T.Resize(img_resize, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    AbsChanMaxScaling(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))   \n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fa5fa3",
   "metadata": {},
   "source": [
    "## Create datasets\n",
    "Load GMNIST train/test dataset using the GMNISTDataset class created above. Then, split the train dataset into two subsets, one to be used as training set (70% of the original train sample) and the other as validation set (the remaining 30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Set train/test datalists\n",
    "filename_train_3chan= os.path.join(dataset_dir, \"train/3chan/datalist_train.json\")\n",
    "filename_test_3chan= os.path.join(dataset_dir, \"test/3chan/datalist_test.json\")\n",
    "\n",
    "# - Read traincv dataset\n",
    "print(\"Read train-cv dataset from file %s ...\" % (filename_train_3chan))\n",
    "dataset_traincv= GMNISTDataset(\n",
    "  metadata_file=filename_train_3chan,\n",
    "  data_path=rundir\n",
    ")\n",
    "\n",
    "# - Read test dataset\n",
    "print(\"Read test dataset from file %s ...\" % (filename_test_3chan))\n",
    "dataset_test= GMNISTDataset(\n",
    "  metadata_file=filename_test_3chan,\n",
    "  transform=transform,\n",
    "  data_path=rundir\n",
    ")\n",
    "dataset_imagenet_test= GMNISTDataset(\n",
    "  metadata_file=filename_test_3chan,\n",
    "  transform=transform_imagenet,\n",
    "  data_path=rundir\n",
    ")\n",
    "\n",
    "# - Split train-cv dataset into train & validation samples\n",
    "print(\"Splitting train-cv dataset in 70% train/30% val subsets...\")\n",
    "generator= torch.Generator().manual_seed(42)\n",
    "subset_train, subset_val= random_split(dataset_traincv, [0.7, 0.3], generator=generator)\n",
    "\n",
    "# - Create train & val datasets from subsets\n",
    "print(\"Creating train & val datasets from subsets ...\")\n",
    "dataset_train= GMNISTDataset(subset=subset_train, transform=transform_train, data_path=rundir)\n",
    "dataset_imagenet_train= GMNISTDataset(subset=subset_train, transform=transform_imagenet_train, data_path=rundir)\n",
    "dataset_val= GMNISTDataset(subset=subset_val, transform=transform, data_path=rundir)\n",
    "dataset_imagenet_val= GMNISTDataset(subset=subset_val, transform=transform_imagenet, data_path=rundir)\n",
    "\n",
    "print(\"#%d entries in train set ...\" % (len(dataset_train)))\n",
    "print(\"#%d entries in validation set ...\" % (len(dataset_val)))\n",
    "print(\"#%d entries in test set ...\" % (len(dataset_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9465ef",
   "metadata": {},
   "source": [
    "### Draw sample images\n",
    "Let's draw some sample images from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605827ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Plot images\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "for i, (tensor_image, target) in islice(enumerate(dataset_train), 16):\n",
    "  label= dataset_train.target2label[target]  \n",
    "  ax = fig.add_subplot(4, 4, i+1)\n",
    "  ax.set_xticks([]); ax.set_yticks([])\n",
    "  im= ax.imshow(tensor_image.permute(1, 2, 0))\n",
    "  ax.set_title(f'{label}', size=15)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ec876",
   "metadata": {},
   "source": [
    "## Create dataloaders\n",
    "We are going to create a dataloader for train, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dbd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "##    CREATE DATA LOADERS\n",
    "###############################\n",
    "# - Create data loaders\n",
    "torch.manual_seed(1)\n",
    "batch_size= 64\n",
    "dataloader_train= torch.utils.data.DataLoader(\n",
    "  dataset_train, \n",
    "  batch_size=batch_size,\n",
    "  shuffle=True, \n",
    "  num_workers=1\n",
    ")\n",
    "dataloader_imagenet_train= torch.utils.data.DataLoader(\n",
    "  dataset_imagenet_train, \n",
    "  batch_size=batch_size,\n",
    "  shuffle=True, \n",
    "  num_workers=1\n",
    ")\n",
    "\n",
    "dataloader_val= torch.utils.data.DataLoader(\n",
    "  dataset_val, \n",
    "  batch_size=batch_size,\n",
    "  shuffle=False, \n",
    "  num_workers=1\n",
    ")\n",
    "dataloader_imagenet_val= torch.utils.data.DataLoader(\n",
    "  dataset_imagenet_val, \n",
    "  batch_size=batch_size,\n",
    "  shuffle=False, \n",
    "  num_workers=1\n",
    ")\n",
    "\n",
    "dataloader_test= torch.utils.data.DataLoader(\n",
    "  dataset_test, \n",
    "  batch_size=8,\n",
    "  shuffle=False, \n",
    "  num_workers=1\n",
    ")\n",
    "dataloader_imagenet_test= torch.utils.data.DataLoader(\n",
    "  dataset_imagenet_test, \n",
    "  batch_size=8,\n",
    "  shuffle=False, \n",
    "  num_workers=1\n",
    ")\n",
    "\n",
    "# - Test min/max\n",
    "imgs, targets = next(iter(dataloader_test))\n",
    "print(\"type(imgs)\")\n",
    "print(type(imgs))\n",
    "print(\"imgs.shape\")\n",
    "print(imgs.shape)\n",
    "\n",
    "data_min= torch.amin(imgs, dim=(2,3))\n",
    "data_max= torch.amax(imgs, dim=(2,3))\n",
    "data_absmax= torch.amax(imgs, dim=(1,2,3))\n",
    "print(\"min: \", data_min)\n",
    "print(\"max: \", data_max)\n",
    "print(\"absmax: \", data_absmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37b099",
   "metadata": {},
   "source": [
    "# CNN classifier\n",
    "We will create two CNN classifiers to perform image classification with the loaded dataset:\n",
    "\n",
    "- ResNet architecture\n",
    "- Custom architecture\n",
    "\n",
    "We will show a complete example for the first architecture. For the custom architecture, we will provide some implementations and hints to allow the user to fully complete the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca78b1",
   "metadata": {},
   "source": [
    "## ResNet classifier\n",
    "Let's define a classifier class to perform GMNIST image classification using a pre-trained model, based on the ResNet architecture. For this, we are going to use the predefined models in `torchvision.models` and the torch `Sequential` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier():\n",
    "  \"\"\" Build a ResNet classifier \"\"\"  \n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    nn_arch: Optional[str] = \"resnet18\",\n",
    "    pretrained_weights: Optional[str] = None,\n",
    "    num_classes: Optional[int] = 4,  \n",
    "    n_dense_layers: Optional[int] = 1,\n",
    "    dense_layer_sizes: Optional[Union[int, list]] = [64],\n",
    "    add_dropout: Optional[bool] = True,\n",
    "    dropout_prob: Optional[float] = 0.5\n",
    "  ):\n",
    "    \"\"\" Initialize class \"\"\"\n",
    "    \n",
    "    self.model= None\n",
    "    self.device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.nn_arch= nn_arch\n",
    "    self.pretrained_weights= pretrained_weights\n",
    "    self.num_classes= num_classes\n",
    "    self.n_dense_layers= n_dense_layers\n",
    "    self.add_dropout= add_dropout\n",
    "    self.dropout_prob= dropout_prob\n",
    "    \n",
    "    # - Set dense layer size per layer\n",
    "    if isinstance(dense_layer_sizes, list):\n",
    "      if len(dense_layer_sizes)!=self.n_dense_layers:\n",
    "        raise Exception(\"dense_layer_sizes list must have length equal to n_dense_layers!\")  \n",
    "      else:\n",
    "        self.dense_layer_sizes= dense_layer_sizes\n",
    "    else:\n",
    "      self.dense_layer_sizes= [dense_layer_sizes]*self.n_dense_layers\n",
    "    \n",
    "    # - Build network\n",
    "    if self.__build_model()<0:\n",
    "      print(\"ERROR: Failed to build model!\")\n",
    "      raise Exception(\"Failed to build model!\")\n",
    "    \n",
    "    # - Move model to device\n",
    "    print(\"Moving model to device %s ...\" % (self.device))\n",
    "    self.model.to(self.device)\n",
    "    \n",
    "  def __build_model(self, pretrained_weights=None):\n",
    "    \"\"\" Create network from pre-defined architecture (e.g. resnet) \"\"\"\n",
    "    \n",
    "    # - Load predefined arch\n",
    "    #   NB: Supported weights for resnet18/34: 'IMAGENET1K_V1'\n",
    "    #       Supported weights for resnet50/101: {'IMAGENET1K_V1','IMAGENET1K_V2'}\n",
    "    if self.nn_arch==\"resnet18\":\n",
    "      self.model = torchvision.models.resnet18(weights=self.pretrained_weights) \n",
    "    elif self.nn_arch==\"resnet34\":\n",
    "      self.model = torchvision.models.resnet34(weights=self.pretrained_weights)  \n",
    "    elif self.nn_arch==\"resnet50\":\n",
    "      self.model = torchvision.models.resnet50(weights=self.pretrained_weights)\n",
    "    elif self.nn_arch==\"resnet101\":\n",
    "      self.model = torchvision.models.resnet101(weights=self.pretrained_weights)\n",
    "    else:\n",
    "      print(\"ERROR: Unsupported nn arch (%s) specified, see torch supported arch below and add it yourself!\")\n",
    "      print(torchvision.models.list_models(module=torchvision.models)) \n",
    "      return -1\n",
    "\n",
    "    # - Define classification head\n",
    "    class_head= torch.nn.Sequential()\n",
    "    \n",
    "    for i in range(self.n_dense_layers):\n",
    "      # - Add dense layer\n",
    "      layer_name= \"fc\" + str(i+1)\n",
    "      class_head.add_module(layer_name, torch.nn.LazyLinear(self.dense_layer_sizes[i]))\n",
    "    \n",
    "      # - Add activation\n",
    "      layer_name= \"relu_fc\" + str(i+1)  \n",
    "      class_head.add_module(layer_name, torch.nn.ReLU())\n",
    "    \n",
    "      # - Add dropout?\n",
    "      if self.add_dropout:\n",
    "        layer_name= \"dropout\" + str(i+1)  \n",
    "        class_head.add_module(layer_name, torch.nn.Dropout(p=self.dropout_prob))\n",
    "    \n",
    "    # - Add dropout if no dense layer specified?\n",
    "    if self.n_dense_layers<=0 and self.add_dropout:\n",
    "      class_head.add_module(\"dropout\", torch.nn.Dropout(p=self.dropout_prob))  \n",
    "    \n",
    "    # - Add output layer\n",
    "    class_head.add_module(\"output\", torch.nn.LazyLinear(self.num_classes))\n",
    "    \n",
    "    # - Override head\n",
    "    self.model.fc = class_head\n",
    "    \n",
    "    return 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76c4aa1",
   "metadata": {},
   "source": [
    "Let's create an instance of the ResNet classifier using a ResNet18 architecture with ImageNet pre-trained weights. The classification head consists of 1 dense hidden layer with 64 neurons. Dropout with 0.5 probability is added in dense layers. Change these settings as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db845a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Create model\n",
    "nn_arch= \"resnet18\"\n",
    "pretrained_weights=\"DEFAULT\"\n",
    "n_dense_layers= 1\n",
    "dense_layer_sizes= [64]\n",
    "add_dropout= True\n",
    "dropout_prob= 0.5\n",
    "\n",
    "classifier_resnet= ResNetClassifier(\n",
    "  nn_arch=nn_arch,\n",
    "  pretrained_weights=pretrained_weights,\n",
    "  num_classes=4,\n",
    "  n_dense_layers=n_dense_layers,  \n",
    "  dense_layer_sizes=dense_layer_sizes,\n",
    "  add_dropout= add_dropout,\n",
    "  dropout_prob= dropout_prob \n",
    ")\n",
    "\n",
    "# - Print model architecture\n",
    "input_shape= (imgs.shape[1], imgs.shape[2], imgs.shape[3])\n",
    "summary(classifier_resnet.model, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d5ce0e",
   "metadata": {},
   "source": [
    "### Train model\n",
    "We define below some methods to run model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2f4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "  def __init__(self):\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.sum = 0\n",
    "    self.count = 0\n",
    "\n",
    "  def update(self, value, n=1):\n",
    "    self.sum += value * n\n",
    "    self.count += n\n",
    "\n",
    "  @property\n",
    "  def avg(self):\n",
    "    return self.sum / self.count if self.count > 0 else 0\n",
    "\n",
    "def run_train(\n",
    "  classifier,\n",
    "  train_dl,\n",
    "  val_dl= None,\n",
    "  num_epochs: Optional[int] = 1, \n",
    "  loss_fn= None,\n",
    "  optimizer= None,  \n",
    "  lr: Optional[float] = 1e-4,\n",
    "  outfile_model=\"model.pth\",\n",
    "):\n",
    "  \"\"\" Train network \"\"\"\n",
    "\n",
    "  # - Get model from classifier\n",
    "  model= classifier.model  \n",
    "\n",
    "  # - Set loss\n",
    "  if loss_fn is None:  \n",
    "    print(\"Setting default CE loss ...\")\n",
    "    loss_fn= torch.nn.CrossEntropyLoss()\n",
    "  \n",
    "  # - Set optimizer\n",
    "  if optimizer is None:\n",
    "    print(\"Setting defaulf Adam optimizer with lr=%f ...\" % (lr))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "  # - Set output model/weights filenames\n",
    "  outfile_model_basenoext= os.path.splitext(os.path.basename(outfile_model))[0]\n",
    "  outfile_model_dir= os.path.dirname(os.path.abspath(outfile_model))\n",
    "  outfile_model_best= os.path.join(outfile_model_dir, outfile_model_basenoext + '_best.pth')\n",
    "  outfile_weights= os.path.join(outfile_model_dir, outfile_model_basenoext + '_weights.pth')\n",
    "  outfile_weights_best= os.path.join(outfile_model_dir, outfile_model_basenoext + '_weights_best.pth')\n",
    "    \n",
    "  # - Init metrics\n",
    "  train_accuracy_metric = torchmetrics.Accuracy(\n",
    "    task=\"multiclass\", \n",
    "    num_classes=classifier.num_classes\n",
    "  ).to(classifier.device)\n",
    "    \n",
    "  train_f1score_metric = torchmetrics.F1Score(\n",
    "    task=\"multiclass\", \n",
    "    num_classes=classifier.num_classes, \n",
    "    average=\"macro\"\n",
    "  ).to(classifier.device)\n",
    "\n",
    "  val_accuracy = None\n",
    "  val_f1score = None\n",
    "  if val_dl is not None:\n",
    "    val_accuracy_metric= torchmetrics.Accuracy(\n",
    "      task=\"multiclass\", \n",
    "      num_classes=classifier.num_classes\n",
    "    ).to(classifier.device)\n",
    "    \n",
    "    val_f1score_metric= torchmetrics.F1Score(\n",
    "      task=\"multiclass\", \n",
    "      num_classes=classifier.num_classes, \n",
    "      average=\"macro\"\n",
    "    ).to(classifier.device)\n",
    "\n",
    "  loss_hist_train = [0] * num_epochs\n",
    "  accuracy_hist_train = [0] * num_epochs\n",
    "  f1score_hist_train = [0] * num_epochs\n",
    "  loss_hist_val = [0] * num_epochs\n",
    "  accuracy_hist_val = [0] * num_epochs\n",
    "  f1score_hist_val = [0] * num_epochs \n",
    "        \n",
    "  # - Training loop\n",
    "  best_val_acc = 0.0\n",
    "    \n",
    "  for epoch in range(num_epochs):\n",
    "    # - Run train batch loop\n",
    "    train_loss, train_acc, train_f1score = train_epoch(\n",
    "      classifier,  \n",
    "      train_dl, \n",
    "      loss_fn, \n",
    "      optimizer,\n",
    "      epoch, \n",
    "      train_accuracy_metric,\n",
    "      train_f1score_metric  \n",
    "    )\n",
    "    loss_hist_train[epoch]= train_loss\n",
    "    accuracy_hist_train[epoch]= train_acc\n",
    "    f1score_hist_train[epoch]= train_f1score\n",
    "        \n",
    "    # - Run validation batch loop?\n",
    "    val_loss= 0.\n",
    "    val_acc= 0.\n",
    "    val_f1score= 0.\n",
    "        \n",
    "    if val_dl is not None:\n",
    "      val_loss, val_acc, val_f1score = validate_epoch(\n",
    "        classifier,  \n",
    "        val_dl, \n",
    "        loss_fn,\n",
    "        epoch, \n",
    "        val_accuracy_metric,\n",
    "        val_f1score_metric  \n",
    "      )\n",
    "      loss_hist_val[epoch]= val_loss\n",
    "      accuracy_hist_val[epoch]= val_acc\n",
    "      f1score_hist_val[epoch]= val_f1score\n",
    "\n",
    "    # - Print metrics  \n",
    "    if val_dl is not None:\n",
    "      print(\"Epoch [%d/%d]: loss=%.4f (val=%.4f), acc=%.4f (val=%.4f), f1=%.4f (val=%.4f)\" % (epoch, num_epochs, train_loss, val_loss, train_acc, val_acc, train_f1score, val_f1score))\n",
    "    else:\n",
    "      print(\"Epoch [%d/%d]: loss=%.4f, acc=%.4f, f1=%.4f\" % (epoch, num_epochs, train_loss, train_acc, train_f1score))\n",
    "        \n",
    "    # - Save best model  \n",
    "    if val_dl is not None and val_acc > best_val_acc:  \n",
    "      best_val_acc = val_acc\n",
    "      print(\"Saving best model at epoch %d (acc_val=%.4f) ...\" % (epoch+1, best_val_acc))  \n",
    "      torch.save(model.state_dict(), outfile_weights_best)  \n",
    "      torch.save(model, outfile_model_best)\n",
    "          \n",
    "  # - Save final model\n",
    "  print(\"Saving final model ...\")  \n",
    "  torch.save(model.state_dict(), outfile_weights) \n",
    "  torch.save(model, outfile_model)\n",
    "    \n",
    "  # - Set metric history\n",
    "  metric_hist= {\n",
    "    \"loss_train\": loss_hist_train,\n",
    "    \"acc_train\": accuracy_hist_train,\n",
    "    \"f1score_train\": f1score_hist_train,\n",
    "    \"loss_val\": loss_hist_val,\n",
    "    \"acc_val\": accuracy_hist_val,\n",
    "    \"f1score_val\": f1score_hist_val\n",
    "  }\n",
    "    \n",
    "  print(\"END TRAIN RUN\")\n",
    "    \n",
    "  return metric_hist\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "  classifier,\n",
    "  dataloader, \n",
    "  criterion, \n",
    "  optimizer, \n",
    "  epoch, \n",
    "  accuracy_metric,\n",
    "  f1score_metric\n",
    "):\n",
    "  \"\"\" Train one epoch \"\"\"\n",
    "    \n",
    "  # - Retrieve model\n",
    "  model= classifier.model\n",
    "\n",
    "  # - Init metrics\n",
    "  model.train()\n",
    "  loss_meter = AverageMeter()\n",
    "  accuracy_metric.reset()\n",
    "  f1score_metric.reset()\n",
    "  progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Training]\", leave=False)\n",
    "\n",
    "  # - Run batch loop\n",
    "  for X_batch, y_batch in progress_bar:\n",
    "    X_batch, y_batch = X_batch.to(classifier.device), y_batch.to(classifier.device)\n",
    "\n",
    "    # - Compute prediction and loss   \n",
    "    outputs = model(X_batch)\n",
    "    loss = criterion(outputs, y_batch)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # - Update loss and accuracy\n",
    "    loss_meter.update(loss.item(), X_batch.size(0))\n",
    "    preds = outputs.argmax(dim=1)\n",
    "    accuracy_metric.update(preds, y_batch)\n",
    "    f1score_metric.update(preds, y_batch)\n",
    "\n",
    "    # - Update progress bar\n",
    "    progress_bar.set_postfix(\n",
    "      loss=loss_meter.avg, \n",
    "      accuracy=accuracy_metric.compute().item(),\n",
    "      f1score=f1score_metric.compute().item()\n",
    "    )\n",
    "\n",
    "  avg_loss = loss_meter.avg\n",
    "  avg_accuracy = accuracy_metric.compute().item()\n",
    "  avg_f1score = f1score_metric.compute().item()\n",
    "\n",
    "  return avg_loss, avg_accuracy, avg_f1score\n",
    "\n",
    "def validate_epoch(\n",
    "  classifier,\n",
    "  dataloader,\n",
    "  criterion,\n",
    "  epoch,\n",
    "  accuracy_metric,\n",
    "  f1score_metric  \n",
    "):\n",
    "  \"\"\" Run validation loop \"\"\"      \n",
    "    \n",
    "  # - Retrieve model\n",
    "  model= classifier.model\n",
    "    \n",
    "  # - Init metrics\n",
    "  model.eval()\n",
    "  loss_meter = AverageMeter()\n",
    "  accuracy_metric.reset()\n",
    "  f1score_metric.reset()\n",
    "  progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1} [Validation]\", leave=False)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    for X_batch, y_batch in progress_bar:\n",
    "      X_batch, y_batch = X_batch.to(classifier.device), y_batch.to(classifier.device)\n",
    "\n",
    "      # - Compute prediction and loss   \n",
    "      outputs = model(X_batch)\n",
    "      loss = criterion(outputs, y_batch)\n",
    "\n",
    "      # - Update loss and accuracy\n",
    "      loss_meter.update(loss.item(), X_batch.size(0))\n",
    "      preds = outputs.argmax(dim=1)\n",
    "      accuracy_metric.update(preds, y_batch)\n",
    "      f1score_metric.update(preds, y_batch)\n",
    "\n",
    "      # - Update progress bar\n",
    "      progress_bar.set_postfix(\n",
    "        loss=loss_meter.avg\n",
    "      )\n",
    "        \n",
    "  avg_loss = loss_meter.avg\n",
    "  avg_accuracy = accuracy_metric.compute().item()\n",
    "  avg_f1score = f1score_metric.compute().item()\n",
    "    \n",
    "  return avg_loss, avg_accuracy, avg_f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090b9762",
   "metadata": {},
   "source": [
    "Let's set some train parameters:\n",
    "\n",
    "- learning rate\n",
    "- number of training epochs\n",
    "- batch size (was defined in data loaders)\n",
    "- loss function\n",
    "\n",
    "You can change them as you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985308e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 10\n",
    "lr= 1e-4\n",
    "loss_fn= torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier_resnet.model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620eb12",
   "metadata": {},
   "source": [
    "We now train the classifier using methods and parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Run train\n",
    "torch.manual_seed(1)\n",
    "outfile_model= os.path.join(rundir, \"resnet_model.pth\")\n",
    "\n",
    "metric_hist_resnet= run_train(\n",
    "  classifier=classifier_resnet,\n",
    "  train_dl=dataloader_imagenet_train,\n",
    "  val_dl=dataloader_imagenet_val,  \n",
    "  num_epochs=num_epochs, \n",
    "  loss_fn=loss_fn,\n",
    "  optimizer=optimizer,  \n",
    "  lr=lr,\n",
    "  outfile_model=outfile_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae86a02",
   "metadata": {},
   "source": [
    "Let's plot some metrics after the training run completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_metric_hist(metric_hist):\n",
    "  \n",
    "  # - Draw losses\n",
    "  x_arr = np.arange(len(metric_hist[\"loss_train\"])) + 1\n",
    "  fig = plt.figure(figsize=(12, 4))\n",
    "  ax = fig.add_subplot(1, 2, 1)\n",
    "  ax.plot(x_arr, metric_hist[\"loss_train\"], '-o', label='Train loss')\n",
    "  ax.plot(x_arr, metric_hist[\"loss_val\"], '--<', label='Validation loss')\n",
    "    \n",
    "  ax.legend(fontsize=8)\n",
    "  ax = fig.add_subplot(1, 2, 2)\n",
    "  \n",
    "  # - Draw accuracy/f1score\n",
    "  ax.set_ylim(0,1)  \n",
    "  ax.plot(x_arr, metric_hist[\"acc_train\"], '-o', label='Train acc.')\n",
    "  ax.plot(x_arr, metric_hist[\"acc_val\"], '--<', label='Validation acc.')\n",
    "  ax.plot(x_arr, metric_hist[\"f1score_train\"], '-*', label='Train F1-score')\n",
    "  ax.plot(x_arr, metric_hist[\"f1score_val\"], '-->', label='Validation F1-score')  \n",
    "  ax.legend(fontsize=8)\n",
    "  ax.set_xlabel('Epoch', size=15)\n",
    "  ax.set_ylabel('Accuracy/F1-score', size=15)\n",
    "\n",
    "  plt.show()  \n",
    "\n",
    "# - Print & plot metrics\n",
    "print(\"== metrics ==\")\n",
    "print(metric_hist_resnet)\n",
    "draw_metric_hist(metric_hist_resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30903c67",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "We evaluate the trained classifier on test data, computing:\n",
    "\n",
    "- classification metrics (accuracy, F1-score, confusion matrix)\n",
    "- plotting feature maps\n",
    "- plotting activation heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(\n",
    "  classifier,\n",
    "  dataloader, \n",
    "  modelfile=\"\", \n",
    "  weightfile=\"\"\n",
    "):\n",
    "  \"\"\" Compute model performances on test data \"\"\"\n",
    "\n",
    "  # - Load model from file?\n",
    "  if modelfile==\"\":\n",
    "    model= classifier.model  \n",
    "  else:\n",
    "    print(\"Loading model from file %s ...\" % (modelfile))    \n",
    "    model= torch.load(modelfile, weights_only=False)\n",
    "      \n",
    "  # - Check for model/dataloader\n",
    "  if model is None:\n",
    "    print(\"ERROR: No model present, cannot run prediction on test set!\")\n",
    "    return None\n",
    "    \n",
    "  # - Load model weights\n",
    "  if weightfile!=\"\":\n",
    "    print(\"Loading model weights from file %s ...\" % (weightfile))\n",
    "    model.load_state_dict(torch.load(weightfile, weights_only=True))\n",
    "      \n",
    "  model.to(classifier.device).eval()\n",
    "    \n",
    "  # - Init metrics    \n",
    "  accuracy_metric= torchmetrics.Accuracy(\n",
    "    task=\"multiclass\", \n",
    "    num_classes=classifier.num_classes\n",
    "  ).to(classifier.device)\n",
    "  accuracy_metric.reset()\n",
    "\n",
    "  f1score_metric= torchmetrics.F1Score(\n",
    "    task=\"multiclass\", \n",
    "    num_classes=classifier.num_classes, \n",
    "    average=\"macro\"\n",
    "  ).to(classifier.device)\n",
    "  f1score_metric.reset()\n",
    "\n",
    "  confusion_matrix_metric= torchmetrics.ConfusionMatrix(\n",
    "    task=\"multiclass\", \n",
    "    num_classes=classifier.num_classes, \n",
    "    normalize=\"true\"\n",
    "  ).to(classifier.device)\n",
    "  confusion_matrix_metric.reset()\n",
    "    \n",
    "  progress_bar = tqdm(dataloader, desc=\"[Test]\", leave=False)\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    for X_batch, y_batch in progress_bar:\n",
    "      X_batch, y_batch = X_batch.to(classifier.device), y_batch.to(classifier.device)\n",
    "\n",
    "      # - Compute prediction and loss   \n",
    "      outputs = model(X_batch)\n",
    "        \n",
    "      # - Update loss and accuracy\n",
    "      preds = outputs.argmax(dim=1)\n",
    "      accuracy_metric.update(preds, y_batch)\n",
    "      f1score_metric.update(preds, y_batch)\n",
    "      confusion_matrix_metric.update(preds, y_batch)\n",
    "        \n",
    "  avg_accuracy = accuracy_metric.compute().item()\n",
    "  avg_f1score = f1score_metric.compute().item()\n",
    "  confusion_matrix= confusion_matrix_metric.compute().numpy()\n",
    "    \n",
    "  metrics= {\n",
    "    \"acc\": avg_accuracy,\n",
    "    \"avg_f1score\": avg_f1score,\n",
    "    \"cm\": confusion_matrix,\n",
    "    \"cm_metric\": confusion_matrix_metric \n",
    "  }\n",
    "    \n",
    "  return metrics  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c040358",
   "metadata": {},
   "source": [
    "Run evaluation and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adeb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightfile= os.path.join(rundir, \"resnet_model_weights.pth\")\n",
    "\n",
    "metrics_resnet_test= run_test(\n",
    "  classifier_resnet,  \n",
    "  dataloader=dataloader_imagenet_test,\n",
    "  weightfile=weightfile \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa265b84",
   "metadata": {},
   "source": [
    "#### Visualizing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Print metrics\n",
    "print(\"== metrics (TEST) ==\")\n",
    "print(metrics_resnet_test)\n",
    "\n",
    "# - Draw confusion matrix\n",
    "fig_, ax_ = metrics_resnet_test[\"cm_metric\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328dc46",
   "metadata": {},
   "source": [
    "#### Visualizing feature maps\n",
    "The activation maps, called feature maps, capture the result of applying the convolutional filters to input, such as the input image or another feature map.\n",
    "\n",
    "The idea of visualizing a feature map for a specific input image would be to understand what features of the input are detected or preserved in the feature maps. The expectation would be that the feature maps close to the input detect small or fine-grained detail, whereas feature maps close to the output of the model capture more general features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166aa43",
   "metadata": {},
   "source": [
    "We define below a method to extract feature maps from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab894a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_maps(\n",
    "  classifier,\n",
    "  image,  \n",
    "  modelfile=\"\", \n",
    "  weightfile=\"\",\n",
    "  return_avg_maps=False\n",
    "):\n",
    "  \"\"\" Extract a list of feature map for a model and input image \"\"\"\n",
    "    \n",
    "  # - Load image on device\n",
    "  #   NB: Transforms are expected to be already applied\n",
    "  if image is None:\n",
    "    print(\"ERROR: Input image is None!\")\n",
    "    return -1\n",
    "  image = image.to(classifier.device)\n",
    "    \n",
    "  # - Load model from file?\n",
    "  if modelfile==\"\":\n",
    "    model= classifier.model  \n",
    "  else:\n",
    "    print(\"Loading model from file %s ...\" % (modelfile))    \n",
    "    model= torch.load(modelfile, weights_only=False)\n",
    "      \n",
    "  # - Check for model/dataloader\n",
    "  if model is None:\n",
    "    print(\"ERROR: No model present, cannot run prediction on test set!\")\n",
    "    return None\n",
    "    \n",
    "  # - Load model weights\n",
    "  if weightfile!=\"\":\n",
    "    print(\"Loading model weights from file %s ...\" % (weightfile))\n",
    "    model.load_state_dict(torch.load(weightfile, weights_only=True))\n",
    "    \n",
    "  model.to(classifier.device).eval()\n",
    "    \n",
    "  # - Extract conv layers\n",
    "  print(\"Extracting all model conv layers ...\")\n",
    "  conv_layers= []\n",
    "  conv_layer_names= []\n",
    "  for name, layer in model.named_modules():\n",
    "    if type(layer) == torch.nn.Conv2d and \"downsample\" not in name:\n",
    "      conv_layers.append(layer)\n",
    "      conv_layer_names.append(name)\n",
    "     \n",
    "  # - Define activations hooks\n",
    "  activations = {}\n",
    "  def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "      activations[name] = output.detach()\n",
    "    return hook\n",
    "    \n",
    "  for name, layer in zip(conv_layer_names, conv_layers):\n",
    "    layer.register_forward_hook(get_activation(name))\n",
    "\n",
    "  # - Forward pass with hooks\n",
    "  output = model(image)\n",
    "    \n",
    "  # - Get activations\n",
    "  feature_maps= []\n",
    "  layer_names= []\n",
    "  print(\"--> fm.shape\")    \n",
    "  for layer_name in conv_layer_names:\n",
    "    layer_output= activations[layer_name].squeeze(0)\n",
    "    print(layer_output.shape)\n",
    "        \n",
    "    if return_avg_maps:\n",
    "      gray_scale = torch.sum(layer_output,0)\n",
    "      gray_scale = gray_scale / layer_output.shape[0]\n",
    "      feature_maps.append(gray_scale.data.cpu().numpy())\n",
    "    else:\n",
    "      feature_maps.append(layer_output.data.cpu().numpy())\n",
    "    \n",
    "    layer_names.append(layer_name)\n",
    "     \n",
    "  return feature_maps, layer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aff99f",
   "metadata": {},
   "source": [
    "We define below a method to draw feature maps from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09512e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_feature_maps(\n",
    "  classifier,\n",
    "  image,\n",
    "  modelfile=\"\", \n",
    "  weightfile=\"\",\n",
    "  images_per_row= 4\n",
    "):\n",
    "  \"\"\" Extract and plot feature maps for an input image \"\"\"      \n",
    "\n",
    "  # - Retrieve feature maps\n",
    "  feature_maps, layer_names= extract_feature_maps(  \n",
    "    classifier,\n",
    "    image,  \n",
    "    modelfile=modelfile, \n",
    "    weightfile=weightfile  \n",
    "  )\n",
    "  if feature_maps is None:\n",
    "    print(\"ERROR: Failed to compute feature maps!\")\n",
    "    return -1\n",
    "  if not feature_maps:\n",
    "    print(\"ERROR: Empty list of feature maps!\")\n",
    "    return -1\n",
    "    \n",
    "  # - Draw feature maps\n",
    "  for layer_name, feature_map in zip(layer_names, feature_maps):\n",
    "    n_features = feature_map.shape[0]\n",
    "    size = feature_map.shape[1]\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "        \n",
    "    for col in range(n_cols):\n",
    "      for row in range(images_per_row):\n",
    "        index= col * images_per_row + row\n",
    "        channel_image = feature_map[index, :, :]\n",
    "        channel_image -= channel_image.mean()\n",
    "        channel_image /= channel_image.std()\n",
    "        channel_image *= 64\n",
    "        channel_image += 128\n",
    "        channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "        display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "  return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e022b",
   "metadata": {},
   "source": [
    "Let's plot the feature maps for a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Take a sample image from the test dataset\n",
    "data_index= 0 # take the first\n",
    "image, target= dataset_imagenet_test[data_index]\n",
    "label= dataset_imagenet_test.target2label[target]\n",
    "image= image.unsqueeze(0)\n",
    "\n",
    "print(\"image\")\n",
    "print(type(image))\n",
    "print(image.shape)\n",
    "\n",
    "# - Draw feature maps\n",
    "weightfile= os.path.join(rundir, \"resnet_model_weights.pth\")\n",
    "\n",
    "draw_feature_maps(\n",
    "  classifier_resnet,  \n",
    "  image,\n",
    "  weightfile=weightfile,\n",
    "  images_per_row=16  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ac87e",
   "metadata": {},
   "source": [
    "#### Visualizing heatmaps of class activation\n",
    "Visualization of class activation map (CAM) is useful to understand which parts of a given image led the model to its final classification decision. It consists of producing heatmaps of class activation over input images. A class activation heatmap is a 2D grid of scores associated with a specific output class, computed for every location in any input image, indicating how important each location is with respect to the class under consideration.\n",
    "\n",
    "The specific implementation we are going to use is the one described in GradCAM: Visual Explanations from Deep Networks via Gradient-based Localization, implemented in this python package. It consists of taking the output feature map of a convolution layer, given an input image, and weighing every channel in that feature map by the gradient of the class with respect to the channel. Intuitively, one way to understand this trick is that you're weighting a spatial map of \"how intensely the input image activates different channels\" by \"how important each channel is with regard to the class\", resulting in a spatial map of \"how intensely the input image activates the class\".\n",
    "\n",
    "Let's plot the heatmaps relative to the last conv layer with respect to the predicted class for a sample of test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b1483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_predictions(\n",
    "  classifier, \n",
    "  dataset,\n",
    "  dataset_gradcam=None,\n",
    "  modelfile=\"\", \n",
    "  weightfile=\"\",\n",
    "  plot_gradcam=True, \n",
    "  gradcam_method=\"gradcam\",\n",
    "  layer_names=[],   \n",
    "  aug_smooth=False,\n",
    "  eigen_smooth=False,\n",
    "  gradcam_alpha=0.5,\n",
    "  apply_heatmap_thr=False,\n",
    "  heatmap_thr=0.7,\n",
    "  plot_gradcam_only=False\n",
    "):\n",
    "  \"\"\" Plot gradCAM on some images \"\"\"\n",
    "    \n",
    "  # - Load model from file?\n",
    "  if modelfile==\"\":\n",
    "    model= classifier.model  \n",
    "  else:\n",
    "    print(\"Loading model from file %s ...\" % (modelfile))    \n",
    "    model= torch.load(modelfile, weights_only=False)\n",
    "      \n",
    "  # - Check for model/dataloader\n",
    "  if model is None:\n",
    "    print(\"ERROR: No model present, cannot run prediction on test set!\")\n",
    "    return None\n",
    "    \n",
    "  # - Load model weights\n",
    "  if weightfile!=\"\":\n",
    "    print(\"Loading model weights from file %s ...\" % (weightfile))\n",
    "    model.load_state_dict(torch.load(weightfile, weights_only=True))\n",
    "    \n",
    "  model.to(classifier.device).eval()\n",
    "    \n",
    "  # - Set dataset to be used for gradcam\n",
    "  if dataset_gradcam is None:\n",
    "    dataset_gradcam= dataset\n",
    "    \n",
    "  # - Init gradCAM \n",
    "  methods = {\n",
    "    \"gradcam\": GradCAM,\n",
    "    \"hirescam\": HiResCAM,\n",
    "    \"scorecam\": ScoreCAM,\n",
    "    \"gradcam++\": GradCAMPlusPlus,\n",
    "    \"ablationcam\": AblationCAM,\n",
    "    \"xgradcam\": XGradCAM,\n",
    "    \"eigencam\": EigenCAM,\n",
    "    \"eigengradcam\": EigenGradCAM,\n",
    "    \"layercam\": LayerCAM,\n",
    "    \"fullgrad\": FullGrad,\n",
    "    #\"fem\": FEM,\n",
    "    \"gradcamelementwise\": GradCAMElementWise,\n",
    "    \"kpcacam\": KPCA_CAM,\n",
    "    #\"shapleycam\": ShapleyCAM\n",
    "  }\n",
    "  cam_algorithm = methods[gradcam_method]\n",
    "  \n",
    "  # - Find RELU layers in model  \n",
    "  print(\"Printing all relu layers in model ...\")\n",
    "  #print(model)\n",
    "  print(find_layer_types_recursive(model, [torch.nn.ReLU]))\n",
    "  target_layers= []\n",
    "  for item in layer_names:\n",
    "    layer= model._modules[item]\n",
    "    print(\"layer\")\n",
    "    print(layer)\n",
    "    target_layers.append(layer)\n",
    "    \n",
    "  # - Set target\n",
    "  #   If targets is None, the highest scoring category (for every member in the batch) will be used\n",
    "  targets = None\n",
    "    \n",
    "  # - Plot images\n",
    "  fig = plt.figure(figsize=(15, 15))\n",
    "      \n",
    "  for i, ((input_img, target), (input_img_gradcam, target_gradcam)) in islice(enumerate(zip(dataset,dataset_gradcam)), 16):    \n",
    "    with torch.enable_grad():\n",
    "      # - Compute model prediction\n",
    "      label= dataset.target2label[target] \n",
    "      print(\"Computing model prediction for image (label=%s, target=%d) ...\" % (label, target))\n",
    "      input_tensor= input_img.unsqueeze(0)\n",
    "      input_tensor_gradcam= input_img_gradcam.unsqueeze(0)                                                       \n",
    "          \n",
    "      pred = model(input_tensor) # logits  \n",
    "      y_pred = torch.argmax(pred).item()\n",
    "      soft_outputs = torch.nn.functional.softmax(pred, dim=1) # pass through softmax\n",
    "      prob_pred, target_pred = soft_outputs.topk(1, dim = 1) # select top probability as prediction\n",
    "      prob_pred= prob_pred.item()\n",
    "      target_pred= target_pred.item()  \n",
    "      label_pred= dataset.target2label[target_pred]\n",
    "        \n",
    "      # - Create image for plot\n",
    "      rgb_img= input_img_gradcam.permute(1, 2, 0).numpy()                                                       \n",
    "      grayscale_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "      # - Compute cam\n",
    "      if plot_gradcam:\n",
    "        with cam_algorithm(model=model, target_layers=target_layers) as cam:\n",
    "          grayscale_cam = cam(\n",
    "            input_tensor=input_tensor_gradcam, \n",
    "            targets=targets,\n",
    "            aug_smooth=aug_smooth,\n",
    "            eigen_smooth=eigen_smooth\n",
    "          )\n",
    "          grayscale_cam = grayscale_cam[0, :]\n",
    "          cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True, image_weight=gradcam_alpha)\n",
    "          cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "          # - Create heatmap\n",
    "          colormap= cv2.COLORMAP_JET\n",
    "          mask= np.copy(grayscale_cam)\n",
    "          heatmap= cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "          heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "          heatmap = np.float32(heatmap) / 255\n",
    "            \n",
    "          alphas= np.ones(grayscale_img.shape)*gradcam_alpha\n",
    "          if apply_heatmap_thr:\n",
    "            alphas[grayscale_cam<heatmap_thr]= 0 # set invisible    \n",
    "    \n",
    "      # - Plot image\n",
    "      ax = fig.add_subplot(4, 4, i+1)\n",
    "      ax.set_xticks([]); ax.set_yticks([])\n",
    "      if not plot_gradcam_only:\n",
    "        ax.imshow(grayscale_img, cmap=\"gray\")\n",
    "            \n",
    "      if plot_gradcam:\n",
    "        if plot_gradcam_only:\n",
    "          ax.imshow(cam_image)  \n",
    "        else:\n",
    "          ax.imshow(heatmap, alpha=alphas)\n",
    "            \n",
    "      ax.set_title(f'{label} \\n pred: {label_pred}, p={prob_pred:.1f})', size=12)\n",
    "  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Run gradCAM on some test data\n",
    "target_layers= [\"layer4\"]\n",
    "weightfile= os.path.join(rundir, \"resnet_model_weights.pth\")\n",
    "\n",
    "plot_sample_predictions(\n",
    "  classifier_resnet,    \n",
    "  dataset=dataset_imagenet_test,\n",
    "  dataset_gradcam=dataset_test,\n",
    "  weightfile=weightfile, \n",
    "  plot_gradcam=True,\n",
    "  layer_names=target_layers,\n",
    "  gradcam_method=\"gradcam\",\n",
    "  aug_smooth=False,\n",
    "  eigen_smooth=False,\n",
    "  gradcam_alpha=0.3,\n",
    "  apply_heatmap_thr=True,\n",
    "  heatmap_thr=0.5 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317ea91",
   "metadata": {},
   "source": [
    "## Custom classifier\n",
    "Let's implement a class that uses the torch `Sequential` class to define a custom network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClassifier():\n",
    "  \"\"\" Build a custom CNN network \"\"\"  \n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    nn_arch: Optional[str] = \"custom\",\n",
    "    num_classes: Optional[int] = 4,\n",
    "    n_conv_layers: Optional[int] = 3,\n",
    "    n_filters: Optional[Union[int, list]] = [8,16,32],\n",
    "    kern_sizes: Optional[Union[int, list]] = [3,5,5],\n",
    "    strides: Optional[Union[int, list]] = [1,1,1],\n",
    "    add_maxpool: Optional[bool] = True,\n",
    "    pool_sizes: Optional[Union[int, list]] = 2,\n",
    "    add_batchnorm: Optional[bool] = True,\n",
    "    n_dense_layers: Optional[int] = 1,\n",
    "    dense_layer_sizes: Optional[Union[int, list]] = [64],\n",
    "    add_dropout: Optional[bool] = True,\n",
    "    dropout_prob: Optional[float] = 0.5\n",
    "  ):\n",
    "    self.model= None\n",
    "    self.device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.nn_arch= nn_arch\n",
    "    self.num_classes= num_classes\n",
    "    self.n_conv_layers= n_conv_layers\n",
    "    self.n_dense_layers= n_dense_layers\n",
    "    self.add_maxpool= add_maxpool\n",
    "    self.add_batchnorm= add_batchnorm\n",
    "    self.add_dropout= add_dropout\n",
    "    self.dropout_prob= dropout_prob\n",
    "    \n",
    "    # - Set number of conv filters per layer\n",
    "    if isinstance(n_filters, list):\n",
    "      if len(n_filters)!=self.n_conv_layers:\n",
    "        raise Exception(\"n_filters list must have length equal to n_conv_layers!\")  \n",
    "      else:\n",
    "        self.n_filters= n_filters\n",
    "    else:\n",
    "      self.n_filters= [n_filters]*self.n_conv_layers \n",
    "    \n",
    "    # - Set conv filter kernel size per layer\n",
    "    if isinstance(kern_sizes, list):\n",
    "      if len(kern_sizes)!=self.n_conv_layers:\n",
    "        raise Exception(\"kern_sizes list must have length equal to n_conv_layers!\")  \n",
    "      else:\n",
    "        self.kern_sizes= kern_sizes\n",
    "    else:\n",
    "      self.kern_sizes= [kern_sizes]*self.n_conv_layers\n",
    "    \n",
    "    # - Set conv filter stride size per layer\n",
    "    if isinstance(strides, list):\n",
    "      if len(strides)!=self.n_conv_layers:\n",
    "        raise Exception(\"strides list must have length equal to n_conv_layers!\")  \n",
    "      else:\n",
    "        self.strides= strides\n",
    "    else:\n",
    "      self.strides= [strides]*self.n_conv_layers\n",
    "    \n",
    "    # - Set conv filter stride size per layer\n",
    "    if isinstance(pool_sizes, list):\n",
    "      if len(pool_sizes)!=self.n_conv_layers:\n",
    "        raise Exception(\"pool_sizes list must have length equal to n_conv_layers!\")  \n",
    "      else:\n",
    "        self.pool_sizes= pool_sizes\n",
    "    else:\n",
    "      self.pool_sizes= [pool_sizes]*self.n_conv_layers\n",
    "    \n",
    "    # - Set dense layer size per layer\n",
    "    if isinstance(dense_layer_sizes, list):\n",
    "      if len(dense_layer_sizes)!=self.n_dense_layers:\n",
    "        raise Exception(\"pool_sizes list must have length equal to n_conv_layers!\")  \n",
    "      else:\n",
    "        self.dense_layer_sizes= dense_layer_sizes\n",
    "    else:\n",
    "      self.dense_layer_sizes= [dense_layer_sizes]*self.n_dense_layers\n",
    "    \n",
    "    # - Build network\n",
    "    print(\"Building NN architecture ...\")\n",
    "    if self.__build_model()<0:\n",
    "      print(\"ERROR: Failed to build nn!\")\n",
    "      raise Exception(\"Failed to build nn!\")\n",
    "    \n",
    "    # - Move model to device\n",
    "    print(\"Moving model to device %s ...\" % (self.device))\n",
    "    self.model.to(self.device)\n",
    "\n",
    "  def __build_model(self):  \n",
    "    \"\"\" Create network \"\"\"\n",
    "    \n",
    "    # - Create model using nn.Sequential class\n",
    "    self.model = torch.nn.Sequential()\n",
    "    \n",
    "    # - Add CNN layers\n",
    "    for i in range(self.n_conv_layers):\n",
    "      # - Add convolution layer  \n",
    "      layer_name= 'conv' + str(i+1)\n",
    "      self.model.add_module(\n",
    "        layer_name,\n",
    "        torch.nn.LazyConv2d(\n",
    "          out_channels=self.n_filters[i],\n",
    "          kernel_size=self.kern_sizes[i],\n",
    "          padding=\"same\",\n",
    "          stride=self.strides[i]\n",
    "        )\n",
    "      )\n",
    "    \n",
    "      # - Add activation\n",
    "      layer_name= 'relu' + str(i+1)\n",
    "      self.model.add_module(layer_name, torch.nn.ReLU())\n",
    "      \n",
    "      # - Add batch normalization?\n",
    "      if self.add_batchnorm:\n",
    "        layer_name= \"bn\" + str(i+1)\n",
    "        self.model.add_module(layer_name, torch.nn.LazyBatchNorm2d())\n",
    "    \n",
    "      # - Add max pool layer?\n",
    "      if self.add_maxpool:\n",
    "        layer_name= 'pool' + str(i+1)\n",
    "        self.model.add_module(layer_name, torch.nn.MaxPool2d(kernel_size=self.pool_sizes[i]))\n",
    "        \n",
    "    # - Flatten layer\n",
    "    self.model.add_module('flatten', torch.nn.Flatten())\n",
    "    \n",
    "    # - Add dense layers\n",
    "    for i in range(self.n_dense_layers):\n",
    "      # - Add dense layer\n",
    "      layer_name= \"fc\" + str(i+1)\n",
    "      self.model.add_module(layer_name, torch.nn.LazyLinear(self.dense_layer_sizes[i]))\n",
    "    \n",
    "      # - Add activation\n",
    "      layer_name= \"relu_fc\" + str(i+1)  \n",
    "      self.model.add_module(layer_name, torch.nn.ReLU())\n",
    "    \n",
    "      # - Add dropout?\n",
    "      if self.add_dropout:\n",
    "        layer_name= \"dropout\" + str(i+1)  \n",
    "        self.model.add_module(layer_name, torch.nn.Dropout(p=self.dropout_prob))\n",
    "    \n",
    "    # - Add dropout if no dense layer specified?\n",
    "    if self.n_dense_layers<=0 and self.add_dropout:\n",
    "      self.model.add_module(\"dropout\", torch.nn.Dropout(p=self.dropout_prob))  \n",
    "    \n",
    "    # - Add output layer\n",
    "    self.model.add_module(\"output\", torch.nn.LazyLinear(self.num_classes))\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc7638",
   "metadata": {},
   "source": [
    "It's time for you to create your custom model.\n",
    "\n",
    "For example, the following code creates an instance of the custom classifier with 3 conv layers, a classification head with 1 dense hidden layer with 64 neurons:\n",
    "\n",
    "```\n",
    "classifier= CustomClassifier(\n",
    "  nn_arch=nn_arch,\n",
    "  n_conv_layers= 3,\n",
    "  n_filters= [8,16,32],\n",
    "  kern_sizes= [3,5,7],\n",
    "  strides= [1,1,1],\n",
    "  add_maxpool= True,\n",
    "  pool_sizes= 2,\n",
    "  add_batchnorm= True,\n",
    "  n_dense_layers= 1,\n",
    "  dense_layer_sizes= [64],\n",
    "  add_dropout= True,\n",
    "  dropout_prob = 0.5\n",
    ")\n",
    "summary(classifier.model, input_shape)\n",
    "```\n",
    "\n",
    "Change layer configuration (filters, stride, add/remove layer) as you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD YOUR CODE HERE ######\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb03d85",
   "metadata": {},
   "source": [
    "### Train model\n",
    "Train the custom classifier using the class `CustomClassifier` defined above and following the steps done with the ResNet classifier.\n",
    "\n",
    "You can re-use the methods defined before:\n",
    "\n",
    "- training method `run_train()`\n",
    "- data loaders (`dataloader_train`, `dataloader_val`)\n",
    "- plotting methods (e.g. `draw_metric_hist`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD YOUR CODE HERE ######\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ddb438",
   "metadata": {},
   "source": [
    "Let's plot some metrics after the training run completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD YOUR CODE HERE ######\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b1b32f",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "Let's load the saved trained model and run inference on test data.\n",
    "You can re-use the methods defined before:\n",
    "\n",
    "- evaluation `run_test()`\n",
    "- plotting methods (`plot_sample_predictions()`, `draw_feature_maps()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD YOUR CODE HERE ######\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3ed53",
   "metadata": {},
   "source": [
    "#### Visualizing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD YOUR CODE HERE ######\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b47ba2",
   "metadata": {},
   "source": [
    "#### Visualizing feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355cbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD YOUR CODE HERE ######\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55b8e8",
   "metadata": {},
   "source": [
    "#### Visualizing heatmaps of class activation\n",
    "Visualize heatmaps using relu layers, e.g. setting `target_layers` in `plot_sample_predictions` to one of the ReLu layer, e.g. `target_layers= [\"relu3\"]` if you defined ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ADD YOUR CODE HERE ######\n",
    "# ...\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usc8-ai-workshop",
   "language": "python",
   "name": "usc8-ai-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
